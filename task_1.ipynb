{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create Perceptron class with activation functions:\n",
    "    \n",
    "    <ul>Sigmoid</ul>\n",
    "    <ul>ReLU</ul>\n",
    "    <ul>Tanh</ul>\n",
    "    \n",
    "2. Train on Iris dataset for binary classification \n",
    "3. Visualize the decision boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function: <br>\n",
    "<img src= \"figures/sigmoid.png\" height=100px>\n",
    "\n",
    "ReLU function: <br>\n",
    "<img src= \"figures/relu.png\" height=100px>\n",
    "\n",
    "tanh function: <br>\n",
    "<img src= \"figures/tann.png\" height=100px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self,  input_dim, activation = 'sigmoid'):\n",
    "        self.activation_func = self._get_activation_function(activation)\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(1 + input_dim)\n",
    "        self.activation_derivative = self._get_activation_derivative(activation)\n",
    "    \n",
    "    \n",
    "\n",
    "    def _get_activation_function(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return lambda x: 1/(1 + np.exp(-x))\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        if activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "\n",
    "    def _get_activation_derivative(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return lambda x: self._get_activation('sigmoid')(x) * (1 - self._get_activation('sigmoid')(x))\n",
    "        elif activation == 'relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        weighted_sum = np.dot(X, self.weights) + self.bias \n",
    "        return self.activation_function(weighted_sum)\n",
    "\n",
    "    def train(self,  X, y, lr=0.01, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            for x_i,y_i in zip(X,y):\n",
    "                #calculate forward pass and error\n",
    "                weighted_sum = self.forward_pass(self, x_i) + bias\n",
    "                y_pred = self.activation_func(weighted_sum)\n",
    "                error = y_i - y_pred\n",
    "\n",
    "                #calcuate derivatives to update weights\n",
    "                der\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
